{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse data with Python Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter Notebook! \n",
    "  \n",
    "Today you'll learn how to import a CSV file into a Jupyter Notebook, and how to analyse already cleaned data. This notebook is part of the course Python for Journalists at [datajournalism.com](https://datajournalism.com/watch/python-for-journalists). The data used originally comes from [the Electoral Commission website](http://search.electoralcommission.org.uk/Search?currentPage=1&rows=10&sort=AcceptedDate&order=desc&tab=1&open=filter&et=pp&isIrishSourceYes=false&isIrishSourceNo=false&date=Reported&from=&to=&quarters=2018Q12&rptPd=3617&prePoll=false&postPoll=false&donorStatus=individual&donorStatus=tradeunion&donorStatus=company&donorStatus=unincorporatedassociation&donorStatus=publicfund&donorStatus=other&donorStatus=registeredpoliticalparty&donorStatus=friendlysociety&donorStatus=trust&donorStatus=limitedliabilitypartnership&donorStatus=impermissibledonor&donorStatus=na&donorStatus=unidentifiabledonor&donorStatus=buildingsociety&register=ni&register=gb&optCols=Register&optCols=IsIrishSource&optCols=ReportingPeriodName), but is edited for training purposes. The  edited dataset is available on the course website and its [Github repo](https://github.com/winnydejong/pythonforjournalists). \n",
    "\n",
    "## About Jupyter Notebooks and Pandas\n",
    "\n",
    "Right now you're looking at a Jupyter Notebook: an interactive, browser based programming environment. You can use these notebooks to program in R, Julia or Python - as you'll be doing later on. Read more about Jupyter Notebook in the [Jupyter Notebook Quick Start Guide](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "  \n",
    "To analyse up our data, we'll be using Python and Pandas. Pandas is an open-source Python library - basically an extra toolkit to go with Python - that is designed for data analysis. Pandas is flexible, easy to use and has lots of useful functions built right in. Read more about Pandas and its features in [the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/). That Pandas functions in ways similar to both spreadsheets and SQL databases (though the latter won't be discussed in this course), makes it beginner friendly. :)  \n",
    "\n",
    "**Notebook shortcuts**  \n",
    "\n",
    "Within Jupyter Notebooks, there are some shortcuts you can use. If you'll be using more notebooks for your data analysis in the future, you'll remember these shortcuts soon enough. :) \n",
    "\n",
    "* `esc` will take you into command mode\n",
    "* `a` will insert cell above\n",
    "* `b` will insert cell below\n",
    "* `shift then tab` will show you the documentation for your code\n",
    "* `shift and enter` will run your cell\n",
    "* ` d d` will delete a cell\n",
    "\n",
    "**Pandas dictionary**\n",
    "\n",
    "* **dataframe**: dataframe is Pandas speak for a table with a labeled y-axis, also known as an index. (The index usually starts at 0.)\n",
    "* **series**: a series is a list, a series can be made of a single column within a dataframe.\n",
    "\n",
    "Before we dive in, a little more about Jupyter Notebooks. Every notebooks is made out of cells. A cell can either contain Markdown text - like this one - or code. In the latter you can execute your code. To see what that means, type the following command in the next cell `print(\"hello world\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the module 'Clean data' from this course, we cleaned up a dataset with donations to political parties in the UK. Now, we're going to analyse the data in that dataset. Let's start by importing the Pandas library, using `import pandas as pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import the cleaned dataset, use `df = pd.read_csv('/path/to/file_with_clean_data.csv')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the data is anything like you'd expect, use `df.head()`, `df.tail()` or `df.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! When we saved the data after cleaning it, the index was saved in an unnamed column. With importing, Pandas added a new index... Let's get rid of the 'Unnamed: 0' column. Drop it like it's hot... `df = df.drop('Unnamed: 0', 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this worked, use `df.head()`, `df.tail()` or `df.sample()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if this looks better, let's get started and analyse some data.\n",
    "\n",
    "# Analyse data\n",
    "\n",
    "## Statistical summary\n",
    "\n",
    "In the module Clean data, you already saw the power of `df.describe()`. This function gives a basic statistical summary of every column in the dataset. It will give you even more information when you tell the function that you want everything included, like this: `df.describe(include='all')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with numeric values, `df.describe()` will give back the most information, here's a full list of the parameters and their meaning: \n",
    "\n",
    "**df.describe() parameters**\n",
    "* **count**: number of values in that column\n",
    "* **unique**: number of unique values in that column\n",
    "* **top**: first value in that column\n",
    "* **freq**: the most common valueâ€™s frequency\n",
    "* **mean**: average\n",
    "* **std**: standard deviation\n",
    "* **min**: minimum value, lowest value in the column\n",
    "* **25%**: first percentile\n",
    "* **50%**: second percentile, this is the same as the median\n",
    "* **75%**: thirth percentile\n",
    "* **max**: maximum value, highest value in the column\n",
    "\n",
    "If a column does not contain numeric value, only those parameters that are applicable are returned. Python gives you NaN-values when that's the case - NaN is short for Not a Number. \n",
    "\n",
    "Notice that 'count' is 300 for every column. This means that every column has a value for every row in the dataset. How do I know? I looked at the total number of rows, using `df.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "Let's try to filter the dataframe based on the value in the Value column. You can do this using `df[df['Value'] > 10000 ]`. This will give you a dataframe with only donations from 10.000 pound or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort\n",
    "Let's try to sort the data. Using the command `df.sort_values(by='column_name')` will sort the dataframe based on the column of your choosing. Sorting by default happens ascending, from small to big. \n",
    "\n",
    "In case you want to see the sorting from big to small, descending, you'll have to type: `df.sort_values(by='column_name', ascending=False)`.\n",
    "\n",
    "Now, let's try to sort the dataframe based on the number in the Value column it's easy to find out who made the biggest donation. \n",
    "\n",
    "The above commands will sort the dataframe by a column, but - since we never asked our notebook to - won't show the data. To sort the data and show us the new order of the top 10, we'll have to combine the command with `.head(10)` like this: `df.sort_values(by='column_name').head(10)`.\n",
    "\n",
    "Now, what would you type if you want to see the 10 smallest donations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the biggest donations made, there are two ways to do that. You could use `df.tail(10)` to see the last 10 rows of the dataframe as it is now. Since the dataframe is ordered from small to big donation, the biggest donations will be in the last 10 rows.\n",
    "\n",
    "Another way of doing this, is using `df.sort_values(by='Value', ascending=False).head(10)`. This would sort the dataframe based on the Value column from big to small. Personally I prefer the latter... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum\n",
    "\n",
    "Wow! There are some big donations in our dataset. If you want to know how much money was donated in total, you need to get the sum of the column Value. Use `df['Value'].sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the receivers of all this donation money. Use `df['RegulatedEntityName'].count()` to count the number of times a regulated entity received a donation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really what we were looking for, right? Using `.count()` gives you the number of values in a column. Not the number of appearances per unique value in the column. \n",
    "\n",
    "You'll need to use `df['RegulatedEntityName'].value_counts()` if you want to know that... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Let's see if you really understand the difference between `.value_counts()` and `.count()` If you want to know how many donors have donated, you should count the values in the DonorName column. Do you use `df['DonorName'].value_counts()` or `df['DonorName'].count()`?\n",
    "\n",
    "When in doubt, try both. Remember: we're using a Jupyter Notebook here. It's a **Notebook**, so you can't go wrong here. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: apparently Ms Jane Mactaggart, Mr Duncan Greenland, and Lord Charles Falconer of Thoroton have donated most often. Let's look into that...\n",
    "\n",
    "## Groupby\n",
    "If you're familiar with Excel, you probably heard of 'pivot tables'. Python Pandas has a function very similar to those pivot tables. \n",
    "\n",
    "Let's start with a small refresher: pivot tables are summaries of a dataset inside a new table. Huh? That's might be a lot to take in. \n",
    "\n",
    "Look at our example: data on donations to political parties in the UK. If we want to know how much each unique donor donated, we are looking for a specific summary of our dataset. To get the anwer to this question: 'How much have Ms Jane Mactaggart, Mr Duncan Greenland, and Lord Charles Falconer of Thoroton donated in total?' We need Pandas to sum up all donation for every donor in the dataframe. In a way, this is a summary of the original dataframe by grouping values by in this case the column DonorName. \n",
    "\n",
    "Using Python this can be done using the group by function. Let's create a new dataframe called donors, that has all donors and the total sum of their donations in there. Use `donors = df.groupby('DonorName')['Value'].sum()`. This is a combination of several functions: group data by 'DonorName', and sum the data in the 'Value' column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if it worked, you'll have to add `donors.head(10)`, otherwise your computer won't know that you actually want to see the result of your effort. :)\n",
    "\n",
    "## Pivot tables\n",
    "\n",
    "But Python has it's own pivot table as well. You can get a similar result in a better looking table using de `df.pivot_table` function. \n",
    "\n",
    "Here's a perfectly fine `.pivot_table` example:\n",
    "`df.pivot_table(values=\"Value\", index=\"DonorName\", columns=\"Year\", aggfunc='sum').sort_values(2018).head(10)`\n",
    "\n",
    "Let's go over this code before running it. What will `df.pivot_table(values=\"Value\", index=\"DonorName\", columns=\"Year\", aggfunc='sum').sort_values(2018).head(10)` actually do? \n",
    "\n",
    "For the dataframe called df, create a pivot table where: \n",
    "- the values in the pivot table should be based on the Value column\n",
    "- the index of the pivot table should be base don the DonorName column, in other words: create a row for every unique value in the DonorName column\n",
    "- create a new column for every unique value in the Year column\n",
    "- aggregate the data that fills up these columns (from the Value column, see?) by summing it for every row. \n",
    "\n",
    "Are you ready to try it yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your data\n",
    "\n",
    "Now that we've put all this work into cleaning our dataset, let's save a copy. Off course Pandas has a nifty command for that too. Use `dataframe.to_csv('filename.csv', encoding='utf8')`. \n",
    "\n",
    "Be ware: use a different name than the filename of the original data file, or it will be overwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to check if a new file was created in your directory, you can use the `pwd` and `ls` commands. At the beginning of this module, we used these commands to print the working directory (`pwd`) and list the content of the working directory (`ls`). \n",
    "\n",
    "First, use `pwd` to see in which folder - also known as directory - you are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `ls` to get a list of all files in this directory. If everything worked your newly saved datafile should be among the files in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
